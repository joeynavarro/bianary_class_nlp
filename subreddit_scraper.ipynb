{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import praw\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I built these functions with help from \n",
    "# I built these funcitons with help from https://medium.com/swlh/scraping-reddit-using-python-57e61e322486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessing the reddit api\n",
    "\n",
    "reddit = praw.Reddit(client_id=\"mTiRlJ3LU5aM8w\",                   # reddit client id\n",
    "                     client_secret=\"cJCF5uuUynS9cRI2s-_DAvr-9ZQ\",  # reddit client secret\n",
    "                     user_agent=\"YouCanDoIt\",                      # reddit user agent name\n",
    "                     username = \"joeynavarro-io\",                  # reddit username\n",
    "                     password = \"$$06082006aC\")                    # reddit password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hot_sub_titles(topic, posts_to_get):\n",
    "        # get 10 hot posts from the MachineLearning subreddit\n",
    "        hot_posts = reddit.subreddit(topic).hot(limit= posts_to_get)\n",
    "        for post in hot_posts:\n",
    "            print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrubber(subreddit_dictionary):    \n",
    "    \n",
    "    \n",
    "        def get_clean_export(topic, posts_to_get):\n",
    "            \n",
    "            #function to change reddit website time format to datetime\n",
    "            def convert_time(input):\n",
    "                times = []\n",
    "    \n",
    "                for time in input:\n",
    "        \n",
    "                    conversion =  datetime.datetime.fromtimestamp(time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    times.append(conversion)\n",
    "                return times\n",
    "        \n",
    "            #obtain the reddits and put them in a dataframe\n",
    "            posts = []\n",
    "            ml_subreddit = reddit.subreddit(topic)\n",
    "\n",
    "            for post in ml_subreddit.hot(limit = posts_to_get):\n",
    "    \n",
    "                posts.append([post.title, post.score, post.id, post.subreddit, \n",
    "                  post.url, post.num_comments, post.selftext, post.created])\n",
    "    \n",
    "            posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', \n",
    "                                    'url', 'num_comments', 'body', 'created'])\n",
    "            \n",
    "            #prints the original shape of the posts dataframe\n",
    "            print(f'{posts.shape} is the shape of the ' + topic + ' subreddit dataframe after scraping.')\n",
    "            \n",
    "            #format the dataframe the way I want\n",
    "            posts['time_created'] = convert_time(posts['created'])\n",
    "            posts.drop('created', axis=1, inplace=True)\n",
    "            \n",
    "            #remove links\n",
    "            #got help from https://stackoverflow.com/questions/45395676/remove-a-url-row-by-row-from-a-large-set-of-text-in-python-panda-dataframe\n",
    "            posts['body'] = posts['body'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "            posts['body'] = posts['body'].str.rstrip('.!? \\n\\t')\n",
    "            posts['body'] = posts['body'].replace('[', '').replace(']', '')\n",
    "            posts['body'] = posts['body'].replace('(', '').replace(')', '')\n",
    "            posts['body'] = posts['body'].str.replace(r'\\W', ' ')\n",
    "            \n",
    "            #remove the empty body fields\n",
    "           \n",
    "            posts = posts[(posts.body != '') & (posts.body != ' ') & (posts.body != 'Title') & (posts.body != np.nan)].reset_index()\n",
    "           \n",
    "            \n",
    "            posts['title'] = posts['title'].str.replace(r'\\W', ' ')\n",
    "            \n",
    "            #change subreddit name to topic\n",
    "            posts['topic'] = posts['subreddit']\n",
    "            \n",
    "            #change column order\n",
    "            posts = posts[['topic', 'time_created', 'title', 'id', 'score', 'num_comments', 'body','url']]\n",
    "            \n",
    "            #add prefix to columns\n",
    "            posts = posts.add_prefix('subreddit_')\n",
    "            \n",
    "            #prints the shape of posts dataframe after removing empty subreddit_body\n",
    "            print(f'>{posts.shape} is the shape of the ' + topic + ' subreddit dataframe after removing subreddit_body NaNs.')\n",
    "            \n",
    "            #turn dataframe into csv\n",
    "            return posts.to_csv(f'./csv_scrapes/{topic}_subreddit.csv', index = False)\n",
    "        \n",
    "        for key, value in subreddit_dictionary.items():\n",
    "    \n",
    "            get_clean_export(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(704, 8) is the shape of the philosophy subreddit dataframe after scraping.\n",
      ">(40, 8) is the shape of the philosophy subreddit dataframe after removing subreddit_body NaNs.\n",
      "(673, 8) is the shape of the rant subreddit dataframe after scraping.\n",
      ">(656, 8) is the shape of the rant subreddit dataframe after removing subreddit_body NaNs.\n",
      "(501, 8) is the shape of the Showerthoughts subreddit dataframe after scraping.\n",
      ">(32, 8) is the shape of the Showerthoughts subreddit dataframe after removing subreddit_body NaNs.\n",
      "(113, 8) is the shape of the 80sRock subreddit dataframe after scraping.\n",
      ">(7, 8) is the shape of the 80sRock subreddit dataframe after removing subreddit_body NaNs.\n",
      "(845, 8) is the shape of the dadjokes subreddit dataframe after scraping.\n",
      ">(841, 8) is the shape of the dadjokes subreddit dataframe after removing subreddit_body NaNs.\n",
      "(992, 8) is the shape of the history subreddit dataframe after scraping.\n",
      ">(912, 8) is the shape of the history subreddit dataframe after removing subreddit_body NaNs.\n",
      "(989, 8) is the shape of the Electricity subreddit dataframe after scraping.\n",
      ">(713, 8) is the shape of the Electricity subreddit dataframe after removing subreddit_body NaNs.\n"
     ]
    }
   ],
   "source": [
    "wanted_subreddits = {\n",
    "    'philosophy' : 1000,\n",
    "    'rant' : 1000,\n",
    "    'Showerthoughts' : 1000,\n",
    "    '80sRock': 1000,\n",
    "    'dadjokes': 1000,\n",
    "    'history' : 1000,\n",
    "    'Electricity':1000\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "scrubber(wanted_subreddits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
